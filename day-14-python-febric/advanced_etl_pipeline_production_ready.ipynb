{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cef81813",
   "metadata": {},
   "source": [
    "\n",
    "# Advanced ETL Pipeline - Production Ready with Database and Cloud Simulation\n",
    "\n",
    "Notebook ini merupakan lanjutan dari pipeline ETL modular sebelumnya. Di sini kita mensimulasikan ETL yang lebih mendekati kebutuhan produksi dengan elemen-elemen seperti:\n",
    "- Struktur modul Python untuk skalabilitas\n",
    "- Integrasi ke database (SQLite sebagai pengganti Postgres/MSSQL untuk simulasi)\n",
    "- Simulasi upload ke cloud (GCS/S3) menggunakan file lokal\n",
    "- Logging untuk monitoring\n",
    "- Config file menggunakan `.env`\n",
    "\n",
    "Struktur Folder Produksi:\n",
    "```\n",
    "project/\n",
    "├── etl/\n",
    "│   ├── extract.py\n",
    "│   ├── transform.py\n",
    "│   ├── load.py\n",
    "├── config/\n",
    "│   ├── settings.py (.env)\n",
    "├── logs/\n",
    "│   └── etl.log\n",
    "├── main.py\n",
    "├── raw_data.csv\n",
    "└── README.md\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c92be0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"customer_id\": [201, 202, 203],\n",
    "    \"name\": [\"Dian\", \"Eka\", \"Fajar\"],\n",
    "    \"join_date\": [\"2022-03-01\", \"2021-07-15\", \"2023-01-10\"],\n",
    "    \"purchase_amount\": [150.0, 220.5, 330.75]\n",
    "})\n",
    "df.to_csv(\"raw_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9059f5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# config/settings.py - menggunakan dotenv\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "DATABASE_URL = os.getenv(\"DATABASE_URL\", \"sqlite:///customer.db\")\n",
    "INPUT_PATH = os.getenv(\"INPUT_PATH\", \"raw_data.csv\")\n",
    "LOG_PATH = os.getenv(\"LOG_PATH\", \"etl.log\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86b091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# etl/extract.py\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "def extract_from_csv(path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        logging.info(\"Extracted data from CSV\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Extract failed: {e}\")\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f561f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# etl/transform.py\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "def transform_customer_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    try:\n",
    "        df['join_date'] = pd.to_datetime(df['join_date'])\n",
    "        df['name'] = df['name'].str.title()\n",
    "        df['purchase_amount'] = df['purchase_amount'].astype(float)\n",
    "        df['days_since_join'] = (pd.Timestamp.now() - df['join_date']).dt.days\n",
    "        logging.info(\"Transformed data successfully\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Transformation failed: {e}\")\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157e0b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# etl/load.py\n",
    "import logging\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "def load_to_database(df, db_url, table_name=\"customers\"):\n",
    "    try:\n",
    "        engine = create_engine(db_url)\n",
    "        df.to_sql(table_name, con=engine, if_exists='replace', index=False)\n",
    "        logging.info(f\"Loaded data to database table '{table_name}'\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load to database: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66727fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# logging setup\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(filename='etl.log',\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3eaf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# main.py - Menjalankan seluruh pipeline\n",
    "\n",
    "from config.settings import INPUT_PATH, DATABASE_URL\n",
    "from etl.extract import extract_from_csv\n",
    "from etl.transform import transform_customer_data\n",
    "from etl.load import load_to_database\n",
    "\n",
    "def run_etl():\n",
    "    df_raw = extract_from_csv(INPUT_PATH)\n",
    "    df_clean = transform_customer_data(df_raw)\n",
    "    load_to_database(df_clean, DATABASE_URL)\n",
    "\n",
    "run_etl()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30169399",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simulasi ETL pipeline langsung di notebook\n",
    "from config.settings import INPUT_PATH, DATABASE_URL\n",
    "from etl.extract import extract_from_csv\n",
    "from etl.transform import transform_customer_data\n",
    "from etl.load import load_to_database\n",
    "\n",
    "df = extract_from_csv(INPUT_PATH)\n",
    "df_clean = transform_customer_data(df)\n",
    "load_to_database(df_clean, DATABASE_URL)\n",
    "\n",
    "print(\"✅ ETL pipeline sukses dijalankan dan dimuat ke database\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}